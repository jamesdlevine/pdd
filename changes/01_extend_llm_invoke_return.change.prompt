<pdd>
Target: pdd/llm_invoke.py

Apply (from ~/pdd):
  pdd change ~/pdd_context_viz/changes/01_extend_llm_invoke_return.change.prompt \
    pdd/llm_invoke.py prompts/llm_invoke_python.prompt \
    --output prompts/llm_invoke_python.prompt
</pdd>

% Context Map Feature - LLM Invoke Extensions

Extend the llm_invoke return dictionary to include additional metrics needed for context map capture.

% Required Changes to Output Dictionary

The current return dictionary has: `result`, `cost`, `model_name`, `thinking_output`.

Add these four new fields:

1. `input_tokens` (int): Prompt token count from `_LAST_CALLBACK_DATA["input_tokens"]`
2. `output_tokens` (int): Completion token count from `_LAST_CALLBACK_DATA["output_tokens"]`
3. `duration_ms` (int): Total invocation time in milliseconds (measure with `time.time()` before/after the LLM call)
4. `provider` (str): Provider name extracted from the model identifier (e.g., "openai" from "openai/gpt-4")

% Implementation Notes

- Token counts are already tracked in `_LAST_CALLBACK_DATA` by the success callback
- For duration, wrap the completion call with timing: `start = time.time()` ... `duration_ms = int((time.time() - start) * 1000)`
- For provider extraction, parse from model name string (split on "/" or use LiteLLM's model info)
- Default to 0 for tokens if callback data unavailable
- Default to "unknown" for provider if extraction fails

% Updated Return Dictionary

```python
return {
    'result': final_result,
    'cost': total_cost,
    'model_name': model_name_litellm,
    'thinking_output': final_thinking if final_thinking else None,
    # New fields for context map:
    'input_tokens': _LAST_CALLBACK_DATA.get("input_tokens", 0),
    'output_tokens': _LAST_CALLBACK_DATA.get("output_tokens", 0),
    'duration_ms': duration_ms,
    'provider': provider,
}
```

% Backward Compatibility

This change is additive. Existing callers that only access `result`, `cost`, `model_name`, or `thinking_output` continue to work unchanged.
