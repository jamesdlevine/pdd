<pdd>
Target: pdd/code_generator_main.py

Apply (from ~/pdd):
  pdd change ~/pdd_context_viz/changes/03b_code_generator_main_instrumentation.change.prompt \
    pdd/code_generator_main.py prompts/code_generator_main_python.prompt \
    --output prompts/code_generator_main_python.prompt
</pdd>

% Context Map Feature - Main Instrumentation Point

Add context map capture to code_generator_main. This is the key instrumentation point because:
1. Preprocessing happens here (not in code_generator.py)
2. Cloud execution bypasses code_generator.py entirely
3. This function has access to prompt file paths and output paths

% Required Changes

Add an optional `context_sampler` parameter to the `code_generator_main` function:

```python
def code_generator_main(
    ctx: click.Context,
    prompt_file: str,
    output: Optional[str],
    original_prompt_file_path: Optional[str],
    force_incremental_flag: bool,
    env_vars: Optional[Dict[str, str]] = None,
    unit_test_file: Optional[str] = None,
    exclude_tests: bool = False,
    context_sampler: Optional['ContextSampler'] = None,  # NEW
) -> Tuple[str, bool, float, str]:
```

% Instrumentation Flow

When `context_sampler` is provided:

1. **Before preprocessing**: Record raw prompt size
   ```python
   if context_sampler:
       context_sampler.record_raw_prompt(len(prompt_content))
   ```

2. **Replace preprocess calls**: Use `preprocess_with_metadata` instead of `preprocess`
   ```python
   from .preprocess import preprocess_with_metadata

   if context_sampler:
       processed_prompt, preprocess_metadata = preprocess_with_metadata(
           prompt_content, recursive=True, double_curly_brackets=False
       )
       context_sampler.record_preprocessing(preprocess_metadata)
   else:
       processed_prompt = pdd_preprocess(prompt_content, ...)
   ```

3. **After LLM call** (both cloud and local paths): Record call metrics
   ```python
   if context_sampler:
       context_sampler.record_llm_call(
           model=result['model_name'],
           provider=result.get('provider', 'unknown'),
           input_tokens=result.get('input_tokens', 0),
           output_tokens=result.get('output_tokens', 0),
           duration_ms=result.get('duration_ms', 0)
       )
   ```

4. **Note**: Finalization happens in sync_orchestration after code_generator_main returns

% Cloud Execution Path

For cloud execution, the response JSON should include token counts and timing. Extract and record them similarly:

```python
if context_sampler and cloud_response:
    context_sampler.record_llm_call(
        model=cloud_response.get('model', 'cloud_model'),
        provider=cloud_response.get('provider', 'cloud'),
        input_tokens=cloud_response.get('prompt_tokens', 0),
        output_tokens=cloud_response.get('completion_tokens', 0),
        duration_ms=cloud_response.get('duration_ms', 0)
    )
```

% Backward Compatibility

When `context_sampler` is None (default), behavior is unchanged. All sampler calls are guarded with `if context_sampler:`.
